import torch
import numpy as np
from collections import defaultdict
from tqdm import tqdm
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Any


def compute_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:
    """
    ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ IoU(Intersection over Union) ê³„ì‚°
    
    Args:
        box1, box2: [x1, y1, x2, y2] í˜•íƒœì˜ ë°”ìš´ë”© ë°•ìŠ¤
    
    Returns:
        IoU ê°’ (0~1 ì‚¬ì´)
    """
    # êµì§‘í•© ì˜ì—­ ê³„ì‚°
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    
    # ê° ë°•ìŠ¤ì˜ ë©´ì  ê³„ì‚°
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    # í•©ì§‘í•© = ë‘ ë©´ì ì˜ í•© - êµì§‘í•©
    union = area1 + area2 - intersection
    
    if union == 0:
        return 0.0
    
    return intersection / union


def compute_ap(recall: np.ndarray, precision: np.ndarray) -> float:
    """
    Average Precision (AP) ê³„ì‚°
    VOC 2010+ ë°©ì‹ (11-point interpolationì´ ì•„ë‹Œ ëª¨ë“  point ì‚¬ìš©)
    
    Args:
        recall: ì •ë ¬ëœ recall ê°’ë“¤
        precision: í•´ë‹¹ recallì—ì„œì˜ precision ê°’ë“¤
    
    Returns:
        Average Precision ê°’
    """
    # recall = 0ê³¼ recall = 1 ì§€ì  ì¶”ê°€
    recall = np.concatenate(([0.0], recall, [1.0]))
    precision = np.concatenate(([0.0], precision, [0.0]))
    
    # precisionì„ ë‹¨ì¡°ê°ì†Œí•˜ë„ë¡ ì¡°ì •
    for i in range(len(precision) - 1, 0, -1):
        precision[i - 1] = max(precision[i - 1], precision[i])
    
    # recallì´ ë³€í•˜ëŠ” ì§€ì ë“¤ ì°¾ê¸°
    indices = np.where(recall[1:] != recall[:-1])[0] + 1
    
    # AP ê³„ì‚° (ë©´ì ì˜ í•©)
    ap = np.sum((recall[indices] - recall[indices - 1]) * precision[indices])
    
    return ap


def evaluate_detection(model, data_loader, device, class_names: List[str], 
                      confidence_threshold: float = 0.5, 
                      iou_thresholds: List[float] = None) -> Dict[str, Any]:
    """
    ê°ì²´ ê²€ì¶œ ëª¨ë¸ì˜ mAP, precision, recall í‰ê°€
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        data_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
        class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ['background', 'class1', 'class2', ...]
        confidence_threshold: ì˜ˆì¸¡ ì‹ ë¢°ë„ ì„ê³„ê°’
        iou_thresholds: IoU ì„ê³„ê°’ë“¤ (Noneì´ë©´ [0.5:0.95:0.05] ì‚¬ìš©)
    
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if iou_thresholds is None:
        iou_thresholds = np.arange(0.5, 1.0, 0.05)  # COCO ìŠ¤íƒ€ì¼: 0.5, 0.55, ..., 0.95
    
    model.eval()
    
    # ê° í´ë˜ìŠ¤ë³„ë¡œ ì˜ˆì¸¡ê³¼ ì •ë‹µì„ ì €ì¥
    all_predictions = defaultdict(list)  # {class_id: [(confidence, box, image_id), ...]}
    all_ground_truths = defaultdict(list)  # {class_id: [(box, image_id), ...]}
    
    print("ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ ì¤‘...")
    
    with torch.no_grad():
        for batch_idx, (images, targets) in enumerate(tqdm(data_loader, desc="Evaluating")):
            images = [image.to(device) for image in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = model(images)
            
            # ë°°ì¹˜ì˜ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì²˜ë¦¬
            for i, (prediction, target) in enumerate(zip(predictions, targets)):
                image_id = batch_idx * len(images) + i
                
                # CPUë¡œ ì´ë™
                prediction = {k: v.cpu() for k, v in prediction.items()}
                target = {k: v.cpu() for k, v in target.items()}
                
                # Ground Truth ìˆ˜ì§‘
                gt_boxes = target['boxes']
                gt_labels = target['labels']
                
                for box, label in zip(gt_boxes, gt_labels):
                    if label > 0:  # ë°°ê²½ í´ë˜ìŠ¤ ì œì™¸
                        all_ground_truths[label.item()].append((box.numpy(), image_id))
                
                # ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘ (confidence threshold ì ìš©)
                pred_boxes = prediction['boxes']
                pred_labels = prediction['labels']
                pred_scores = prediction['scores']
                
                # ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë†’ì€ ì˜ˆì¸¡ë§Œ ì„ íƒ
                high_conf_mask = pred_scores >= confidence_threshold
                
                for box, label, score in zip(pred_boxes[high_conf_mask], 
                                           pred_labels[high_conf_mask], 
                                           pred_scores[high_conf_mask]):
                    if label > 0:  # ë°°ê²½ í´ë˜ìŠ¤ ì œì™¸
                        all_predictions[label.item()].append((score.item(), box.numpy(), image_id))
    
    print("í‰ê°€ ì§€í‘œ ê³„ì‚° ì¤‘...")
    
    # ê° IoU ì„ê³„ê°’ê³¼ í´ë˜ìŠ¤ì— ëŒ€í•´ AP ê³„ì‚°
    results = {
        'per_class_ap': {},
        'per_class_metrics': {},
        'mAP': {},
        'overall_metrics': {}
    }
    
    # í´ë˜ìŠ¤ë³„ AP ê³„ì‚°
    for class_id in range(1, len(class_names)):  # ë°°ê²½ í´ë˜ìŠ¤(0) ì œì™¸
        class_name = class_names[class_id]
        
        if class_id not in all_ground_truths:
            print(f"í´ë˜ìŠ¤ '{class_name}'ì— ëŒ€í•œ Ground Truthê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        if class_id not in all_predictions:
            print(f"í´ë˜ìŠ¤ '{class_name}'ì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            results['per_class_ap'][class_name] = {f'AP@{iou:.2f}': 0.0 for iou in iou_thresholds}
            continue
        
        # ì˜ˆì¸¡ì„ confidence ìˆœìœ¼ë¡œ ì •ë ¬
        predictions = sorted(all_predictions[class_id], key=lambda x: x[0], reverse=True)
        ground_truths = all_ground_truths[class_id]
        
        # ê° IoU ì„ê³„ê°’ì— ëŒ€í•´ AP ê³„ì‚°
        class_aps = {}
        
        for iou_threshold in iou_thresholds:
            # ê° ì´ë¯¸ì§€ë³„ë¡œ GT ë§¤ì¹­ ì—¬ë¶€ ì¶”ì  - ìˆ˜ì •ëœ ë²„ì „
            gt_matched = {}
            
            # ê° ì´ë¯¸ì§€ë³„ GT ê°œìˆ˜ ë¯¸ë¦¬ ê³„ì‚°
            image_gt_counts = defaultdict(int)
            for _, gt_image_id in ground_truths:
                image_gt_counts[gt_image_id] += 1
            
            # ê° ì´ë¯¸ì§€ë³„ë¡œ ë§¤ì¹­ ë°°ì—´ ì´ˆê¸°í™”
            for image_id, count in image_gt_counts.items():
                gt_matched[image_id] = np.zeros(count, dtype=bool)
            
            true_positives = []
            false_positives = []
            
            for confidence, pred_box, pred_image_id in predictions:
                # ê°™ì€ ì´ë¯¸ì§€ì˜ GTë“¤ê³¼ ë¹„êµ - ì¸ë±ìŠ¤ ë§¤í•‘ ìˆ˜ì •
                image_gts = []
                local_idx = 0
                for global_idx, (gt_box, gt_image_id) in enumerate(ground_truths):
                    if gt_image_id == pred_image_id:
                        image_gts.append((gt_box, local_idx))
                        local_idx += 1
                
                best_iou = 0
                best_gt_local_idx = -1
                
                for gt_box, local_gt_idx in image_gts:
                    iou = compute_iou(torch.tensor(pred_box), torch.tensor(gt_box))
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_local_idx = local_gt_idx
                
                # ë§¤ì¹­ í™•ì¸ ë° ì—…ë°ì´íŠ¸
                if (best_iou >= iou_threshold and 
                    pred_image_id in gt_matched and 
                    best_gt_local_idx < len(gt_matched[pred_image_id]) and
                    not gt_matched[pred_image_id][best_gt_local_idx]):
                    # True Positive
                    true_positives.append(1)
                    false_positives.append(0)
                    gt_matched[pred_image_id][best_gt_local_idx] = True
                else:
                    # False Positive
                    true_positives.append(0)
                    false_positives.append(1)
            
            # ëˆ„ì í•© ê³„ì‚°
            tp_cumsum = np.cumsum(true_positives)
            fp_cumsum = np.cumsum(false_positives)
            
            # Precisionê³¼ Recall ê³„ì‚°
            num_gt = len(ground_truths)
            recalls = tp_cumsum / max(num_gt, 1)
            precisions = tp_cumsum / np.maximum(tp_cumsum + fp_cumsum, 1e-8)
            
            # AP ê³„ì‚°
            ap = compute_ap(recalls, precisions)
            class_aps[f'AP@{iou_threshold:.2f}'] = ap
            
        results['per_class_ap'][class_name] = class_aps
        
        # í´ë˜ìŠ¤ë³„ ì „ì²´ ë©”íŠ¸ë¦­ (IoU@0.5 ê¸°ì¤€)
        if predictions and ground_truths:
            final_precision = precisions[-1] if len(precisions) > 0 else 0.0
            final_recall = recalls[-1] if len(recalls) > 0 else 0.0
            f1_score = 2 * final_precision * final_recall / max(final_precision + final_recall, 1e-8)
            
            results['per_class_metrics'][class_name] = {
                'precision': final_precision,
                'recall': final_recall,
                'f1_score': f1_score,
                'num_predictions': len(predictions),
                'num_ground_truths': len(ground_truths)
            }
    
    # ì „ì²´ mAP ê³„ì‚°
    for iou_threshold in iou_thresholds:
        iou_key = f'AP@{iou_threshold:.2f}'
        aps = []
        for class_name in results['per_class_ap']:
            if iou_key in results['per_class_ap'][class_name]:
                aps.append(results['per_class_ap'][class_name][iou_key])
        
        results['mAP'][iou_key] = np.mean(aps) if aps else 0.0
    
    # mAP@0.5:0.95 (COCO ìŠ¤íƒ€ì¼)
    results['mAP']['mAP@0.5:0.95'] = np.mean([results['mAP'][f'AP@{iou:.2f}'] for iou in iou_thresholds])
    
    # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
    all_precisions = [metrics['precision'] for metrics in results['per_class_metrics'].values()]
    all_recalls = [metrics['recall'] for metrics in results['per_class_metrics'].values()]
    all_f1s = [metrics['f1_score'] for metrics in results['per_class_metrics'].values()]
    
    results['overall_metrics'] = {
        'mean_precision': np.mean(all_precisions) if all_precisions else 0.0,
        'mean_recall': np.mean(all_recalls) if all_recalls else 0.0,
        'mean_f1_score': np.mean(all_f1s) if all_f1s else 0.0,
        'total_predictions': sum([metrics['num_predictions'] for metrics in results['per_class_metrics'].values()]),
        'total_ground_truths': sum([metrics['num_ground_truths'] for metrics in results['per_class_metrics'].values()])
    }
    
    return results


def print_evaluation_results(results: Dict[str, Any]):
    """í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ê°ì²´ ê²€ì¶œ í‰ê°€ ê²°ê³¼")
    print("="*80)
    
    # ì „ì²´ mAP
    print("\nğŸ“Š ì „ì²´ mAP:")
    print(f"  mAP@0.5:0.95: {results['mAP']['mAP@0.5:0.95']:.4f}")
    print(f"  mAP@0.5:     {results['mAP'].get('AP@0.50', 0.0):.4f}")
    print(f"  mAP@0.75:    {results['mAP'].get('AP@0.75', 0.0):.4f}")
    
    # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
    print(f"\nğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
    overall = results['overall_metrics']
    print(f"  í‰ê·  Precision: {overall['mean_precision']:.4f}")
    print(f"  í‰ê·  Recall:    {overall['mean_recall']:.4f}")
    print(f"  í‰ê·  F1-Score:  {overall['mean_f1_score']:.4f}")
    print(f"  ì´ ì˜ˆì¸¡ ìˆ˜:     {overall['total_predictions']}")
    print(f"  ì´ Ground Truth: {overall['total_ground_truths']}")
    
    # í´ë˜ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼
    print(f"\nğŸ“‹ í´ë˜ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼:")
    print("-"*80)
    print(f"{'í´ë˜ìŠ¤':<15} {'AP@0.5':<8} {'AP@0.75':<9} {'Precision':<10} {'Recall':<8} {'F1-Score':<8}")
    print("-"*80)
    
    for class_name in results['per_class_ap']:
        ap_data = results['per_class_ap'][class_name]
        metrics_data = results.get('per_class_metrics', {}).get(class_name, {})
        
        ap_50 = ap_data.get('AP@0.50', 0.0)
        ap_75 = ap_data.get('AP@0.75', 0.0)
        precision = metrics_data.get('precision', 0.0)
        recall = metrics_data.get('recall', 0.0)
        f1_score = metrics_data.get('f1_score', 0.0)
        
        print(f"{class_name:<15} {ap_50:<8.4f} {ap_75:<9.4f} {precision:<10.4f} {recall:<8.4f} {f1_score:<8.4f}")
    
    print("-"*80)


def plot_evaluation_results(results: Dict[str, Any], save_path: str = None):
    """í‰ê°€ ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ê°ì²´ ê²€ì¶œ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼', fontsize=16, fontweight='bold')
    
    # 1. í´ë˜ìŠ¤ë³„ AP@0.5
    if results['per_class_ap']:
        classes = list(results['per_class_ap'].keys())
        ap_values = [results['per_class_ap'][cls].get('AP@0.50', 0.0) for cls in classes]
        
        axes[0, 0].bar(classes, ap_values, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('í´ë˜ìŠ¤ë³„ AP@0.5')
        axes[0, 0].set_ylabel('Average Precision')
        axes[0, 0].tick_params(axis='x', rotation=45)
        axes[0, 0].grid(True, alpha=0.3)
    
    # 2. í´ë˜ìŠ¤ë³„ Precision, Recall, F1-Score
    if results['per_class_metrics']:
        classes = list(results['per_class_metrics'].keys())
        precisions = [results['per_class_metrics'][cls]['precision'] for cls in classes]
        recalls = [results['per_class_metrics'][cls]['recall'] for cls in classes]
        f1_scores = [results['per_class_metrics'][cls]['f1_score'] for cls in classes]
        
        x = np.arange(len(classes))
        width = 0.25
        
        axes[0, 1].bar(x - width, precisions, width, label='Precision', alpha=0.7)
        axes[0, 1].bar(x, recalls, width, label='Recall', alpha=0.7)
        axes[0, 1].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.7)
        
        axes[0, 1].set_title('í´ë˜ìŠ¤ë³„ Precision, Recall, F1-Score')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(classes, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    
    # 3. IoU ì„ê³„ê°’ë³„ mAP
    iou_keys = [k for k in results['mAP'].keys() if k.startswith('AP@')]
    if iou_keys:
        iou_thresholds = [float(k.split('@')[1]) for k in iou_keys]
        map_values = [results['mAP'][k] for k in iou_keys]
        
        axes[1, 0].plot(iou_thresholds, map_values, 'o-', linewidth=2, markersize=6)
        axes[1, 0].set_title('IoU ì„ê³„ê°’ë³„ mAP')
        axes[1, 0].set_xlabel('IoU Threshold')
        axes[1, 0].set_ylabel('mAP')
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_xlim(0.5, 0.95)
    
    # 4. ì „ì²´ ì„±ëŠ¥ ìš”ì•½ (ë„ë„› ì°¨íŠ¸)
    overall = results['overall_metrics']
    metrics = ['Precision', 'Recall', 'F1-Score']
    values = [overall['mean_precision'], overall['mean_recall'], overall['mean_f1_score']]
    colors = ['#ff9999', '#66b3ff', '#99ff99']
    
    wedges, texts, autotexts = axes[1, 1].pie(values, labels=metrics, colors=colors, autopct='%1.3f',
                                             startangle=90, wedgeprops=dict(width=0.5))
    axes[1, 1].set_title('ì „ì²´ ì„±ëŠ¥ ìš”ì•½')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"í‰ê°€ ê²°ê³¼ ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.show()


# ====================================================================
# trainer_fasterrcnn.pyì—ì„œ ì‚¬ìš©í•  í†µí•© í‰ê°€ í•¨ìˆ˜
# ====================================================================

def evaluate_with_metrics(model, data_loader, device, class_names, 
                         confidence_threshold=0.5, compute_detailed_metrics=True):
    """
    ê¸°ì¡´ evaluate í•¨ìˆ˜ë¥¼ í™•ì¥í•˜ì—¬ lossì™€ detection metricsë¥¼ ëª¨ë‘ ê³„ì‚°
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        data_loader: ê²€ì¦ ë°ì´í„° ë¡œë”  
        device: ë””ë°”ì´ìŠ¤
        class_names: ['background', 'class1', 'class2', ...] í˜•íƒœì˜ í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        confidence_threshold: ì˜ˆì¸¡ ì‹ ë¢°ë„ ì„ê³„ê°’
        compute_detailed_metrics: ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì—¬ë¶€ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    
    Returns:
        avg_loss: í‰ê·  ê²€ì¦ ì†ì‹¤
        detailed_results: mAP, precision, recall ë“± ìƒì„¸ ê²°ê³¼ (Noneì¼ ìˆ˜ ìˆìŒ)
    """
    model.eval()
    total_loss = 0.0
    detailed_results = None
    
    # Loss ê³„ì‚°
    print("ê²€ì¦ ì†ì‹¤ ê³„ì‚° ì¤‘...")
    with torch.no_grad():
        for images, targets in tqdm(data_loader, desc="Computing Loss"):
            images = [image.to(device) for image in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            # Loss ê³„ì‚°ì„ ìœ„í•´ ì¼ì‹œì ìœ¼ë¡œ train ëª¨ë“œ
            model.train()
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            total_loss += losses.item()
            model.eval()

    avg_loss = total_loss / len(data_loader)
    print(f"Validation Loss: {avg_loss:.4f}")
    
    # ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° (ì„ íƒì )
    if compute_detailed_metrics:
        print("ìƒì„¸ ê²€ì¶œ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        detailed_results = evaluate_detection(
            model, data_loader, device, class_names, confidence_threshold
        )
        print_evaluation_results(detailed_results)
    
    return avg_loss, detailed_results


# ====================================================================
# trainer_fasterrcnn.pyì˜ main í•¨ìˆ˜ ìˆ˜ì • ì˜ˆì‹œ
# ====================================================================

def modified_main_loop_example():
    """
    ê¸°ì¡´ trainer_fasterrcnn.pyì˜ main í•¨ìˆ˜ì—ì„œ ìˆ˜ì •í•  ë¶€ë¶„ ì˜ˆì‹œ
    """
    
    # í´ë˜ìŠ¤ ì´ë¦„ ìƒì„± (ê¸°ì¡´ ì½”ë“œì—ì„œ)
    class_names = ['background'] + [name for name in train_dataset.sequential_label_to_original_name.values()]
    
    # í•™ìŠµ ë£¨í”„ì—ì„œ ê²€ì¦ ë¶€ë¶„ ìˆ˜ì •
    for epoch in range(start_epoch, args.num_epochs):
        # í•™ìŠµ
        train_loss = train_one_epoch(
            model, optimizer, train_loader, args.device, epoch, args.print_freq
        )
        train_losses.append(train_loss)

        # ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì—¬ë¶€ ê²°ì • (ì˜ˆ: 5 ì—í¬í¬ë§ˆë‹¤)
        compute_detailed = (epoch + 1) % 5 == 0 or epoch == args.num_epochs - 1
        
        # ê²€ì¦ (ìˆ˜ì •ëœ í•¨ìˆ˜ ì‚¬ìš©)
        val_loss, detailed_results = evaluate_with_metrics(
            model, val_loader, args.device, class_names, 
            confidence_threshold=args.confidence_threshold,
            compute_detailed_metrics=compute_detailed
        )
        val_losses.append(val_loss)
        
        # ìƒì„¸ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹œê°í™”
        if detailed_results and args.visualize_predictions:
            plot_evaluation_results(
                detailed_results, 
                save_path=os.path.join(args.checkpoint_dir, f'metrics_epoch_{epoch+1}.png')
            )

        # ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼...