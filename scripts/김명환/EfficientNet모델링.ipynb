{"cells":[{"cell_type":"markdown","metadata":{"id":"G3tb146HKUGS"},"source":["# [초급 프로젝트] 4팀_김명환"]},{"cell_type":"markdown","metadata":{"id":"hmaVeBaGKUGW"},"source":["---\n","---"]},{"cell_type":"markdown","metadata":{"id":"KzN8SzLMKgH1"},"source":["# 프로그래밍"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECzUDN-OzK99"},"outputs":[],"source":["!pip install -q gdown\n","!pip install -q albumentations\n","!pip install -q ultralytics\n","!pip install -q -U ultralytics\n","!pip install -q nbformat\n","!pip install -q roboflow\n","!pip install -q opencv-python\n","!pip install -q opencv-python-headless\n","!pip install -q wandb\n","!pip install -q timm\n","!pip install -q torchvision\n","#!pip install -q torch torchvision tqdm pillow matplotlib\n","\n","print(\"로딩완료\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAK5c2b2zK9-"},"outputs":[],"source":["!wandb login 86a7b8c07184b2efdfb116546a17b1905e41cb5d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47-Qf8TYKUGW"},"outputs":[],"source":["# 기본 라이브러리 (중복 제거 및 정리)\n","\n","# --- Scikit-learn: 데이터 전처리, 모델, 평가 ---\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import (\n","    fetch_california_housing, load_iris, make_moons, make_circles,\n","    load_breast_cancer, load_wine\n",")\n","from sklearn import datasets\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, mean_squared_error, average_precision_score\n","\n","# --- 이미지 처리 ---\n","import cv2\n","from PIL import Image, ImageFilter, ImageDraw\n","import albumentations as A\n","\n","# --- PyTorch: 딥러닝 관련 ---\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","# 문제 있는 v2 import 제거하고 필요시에만 개별적으로 import\n","# from torchvision.transforms import v2, functional as TF\n","from torchvision.transforms import functional as TF\n","from torchvision.datasets import CocoDetection\n","from torch.nn import CrossEntropyLoss\n","from collections import OrderedDict\n","\n","# --- COCO 데이터셋 관련 ---\n","from pycocotools.coco import COCO\n","from pycocotools import mask as coco_mask\n","\n","# --- 딥러닝 모델 ---\n","import timm\n","\n","# --- 기본 라이브러리 ---\n","import os\n","import sys\n","import re\n","import csv\n","import copy\n","import json\n","import math\n","import random\n","import yaml\n","import shutil\n","import requests\n","import xml.etree.ElementTree as ET\n","from pathlib import Path\n","\n","# --- 데이터 분석 및 시각화 ---\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","# --- 시간 관련 ---\n","from datetime import datetime, timezone, timedelta\n","import pytz\n","\n","# --- 진행률 표시 ---\n","import IPython.display\n","from tqdm.notebook import tqdm\n","\n","# --- 시간대 설정 ---\n","__kst = pytz.timezone('Asia/Seoul')\n","\n","# --- GPU 설정 ---\n","__device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","__device_cpu = torch.device('cpu')\n","\n","# --- 재현 가능한 결과를 위한 시드 설정 ---\n","np.random.seed(42)\n","torch.manual_seed(42)\n","if __device.type == 'cuda':\n","    torch.cuda.manual_seed_all(42)\n","\n","print(f\"라이브러리 로드 완료 사용장치: {__device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDTmqQCCrBWm"},"outputs":[],"source":["from urllib.request import urlretrieve; urlretrieve(\"https://raw.githubusercontent.com/c0z0c/jupyter_hangul/refs/heads/beta/helper_c0z0c_dev.py\", \"helper_c0z0c_dev.py\")\n","import importlib\n","import helper_c0z0c_dev as helper\n","importlib.reload(helper)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WE6336hF11C5"},"outputs":[],"source":["import os, sys\n","from pathlib import Path\n","\n","utils_dir = None\n","if helper.is_colab:\n","    utils_dir = \"/content/drive/MyDrive/codeit_ai_health_eat/src/python_modules/utils\"\n","else:\n","    utils_dir = os.path.join(Path.cwd().drive + '\\\\', 'GoogleDrive', \"codeit_ai_health_eat\", \"src\", \"python_modules\", \"utils\")\n","\n","print(\"utils_dir:\", utils_dir)\n","\n","sys.path.append(str(utils_dir))\n","print(\"sys.path:\", sys.path)\n","import importlib\n","import health_ea_utils as heu\n","importlib.reload(heu)\n","from health_ea_utils import *\n","\n","print(\"helper.__file__:\", helper.__file__)\n","print(\"health_ea_utils.__file__:\", heu.__file__)\n"]},{"cell_type":"markdown","metadata":{"id":"20rBdRxvKUGZ"},"source":["# 1. 학습용 데이타 다운로드 및 압축 풀기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3JHrVMkzK9_"},"outputs":[],"source":["def get_tqdm_kwargs():\n","    \"\"\"Widget 오류를 방지하는 안전한 tqdm 설정\"\"\"\n","    return {\n","        'disable': False,\n","        'leave': True,\n","        'file': sys.stdout,\n","        'ascii': True,  # ASCII 문자만 사용\n","        'dynamic_ncols': False,\n","#        'ncols': 80  # 고정 폭\n","    }\n","\n","def drive_root():\n","    root_path = os.path.join(\"D:\\\\\", \"GoogleDrive\")\n","    if helper.is_colab:\n","        root_path = os.path.join(\"/content/drive/MyDrive\")\n","    return root_path\n","\n","def get_path_modeling(add_path = None):\n","    modeling_path = \"modeling_yolo\"\n","    path = os.path.join(drive_root(),modeling_path)\n","    if add_path is not None:\n","        path = os.path.join(path,add_path)\n","    return path\n","\n","def get_path_modeling_release(add_path = None):\n","    modeling_path = \"modeling_yolo\"\n","    path = os.path.join(drive_root(),modeling_path)\n","    if add_path is not None:\n","        path = os.path.join(path,add_path)\n","    return path\n","\n","def print_dir_tree(root, max_depth=2, list_count=3, indent=\"\"):\n","    import os\n","    if max_depth < 0:\n","        return\n","    try:\n","        items = os.listdir(root)\n","    except Exception as e:\n","        print(indent + f\"[Error] {e}\")\n","        return\n","\n","    img_count = len([f for f in os.listdir(root) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.xml', '.inf', '.txt'))])\n","    for item in items:\n","        path = os.path.join(root, item)\n","        if os.path.isdir(path):\n","            print(indent + \"|-- \"+ item)\n","            # 이미지 파일 개수만 출력\n","            img_count = len([f for f in os.listdir(path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.xml', '.inf', '.txt'))])\n","            if img_count > list_count:\n","                print(indent + \"   \"+ f\"[데이터파일: {img_count}개]\")\n","            print_dir_tree(root=path, max_depth=max_depth-1, list_count=list_count, indent=indent + \"   \")\n","        else:\n","            if list_count < img_count and item.lower().endswith(('.jpg', '.jpeg', '.png', '.xml', '.inf', '.txt')):\n","                continue\n","            print(indent + \"|-- \"+ item)\n","\n","def save_model_dict(model, path, pth_name, kwargs=None):\n","    \"\"\"모델 state_dict와 추가 정보를 저장\"\"\"\n","    def safe_makedirs(path):\n","        \"\"\"안전한 디렉토리 생성\"\"\"\n","        if os.path.exists(path) and not os.path.isdir(path):\n","            os.remove(path)  # 파일이면 삭제\n","        os.makedirs(path, exist_ok=True)\n","\n","    # 디렉토리 생성\n","    safe_makedirs(path)\n","\n","    # 모델 구조 정보 추출\n","    model_info = {\n","        'class_name': model.__class__.__name__,\n","        'init_args': {},\n","        'str': str(model),\n","        'repr': repr(model),\n","        'modules': [m.__class__.__name__ for m in model.modules()],\n","    }\n","\n","    # 생성자 인자 자동 추출(가능한 경우)\n","    if hasattr(model, '__dict__'):\n","        for key in ['in_ch', 'base_ch', 'num_classes', 'out_ch']:\n","            if hasattr(model, key):\n","                model_info['init_args'][key] = getattr(model, key)\n","\n","    # kwargs 처리\n","    extra_info = {}\n","    if kwargs is not None:\n","        if isinstance(kwargs, str):\n","            extra_info = json.loads(kwargs)\n","        elif isinstance(kwargs, dict):\n","            extra_info = kwargs\n","\n","    model_info.update(extra_info)\n","\n","    # 저장할 dict 구성\n","    save_dict = {\n","        'model_state': model.state_dict(),\n","        'class_name': model.__class__.__name__,\n","        'model_info': model_info,\n","    }\n","\n","    save_path = os.path.join(path, f\"{pth_name}.pth\")\n","    torch.save(save_dict, save_path)\n","    return save_path\n","\n","def load_model_dict(path, pth_name=None):\n","    \"\"\"\n","    save_model_dict로 저장한 모델을 불러오는 함수\n","    반환값: (model_state, model_info)\n","    \"\"\"\n","    import torch\n","    load_path = path\n","    if pth_name is not None:\n","        load_path = os.path.join(path, f\"{pth_name}.pth\")\n","    checkpoint = torch.load(load_path, map_location='cpu', weights_only=False)  # <-- 여기 추가\n","    model_state = checkpoint.get('model_state')\n","    model_info = checkpoint.get('model_info')\n","    model_info['file_name'] = os.path.basename(load_path)\n","    return model_state, model_info\n","\n","\n","def search_pth_files(base_path):\n","    \"\"\"\n","    입력된 경로의 하위 폴더들에서 pth 파일들을 검색\n","    \"\"\"\n","    pth_files = []\n","\n","    if not os.path.exists(base_path):\n","        print(f\"경로가 존재하지 않습니다: {base_path}\")\n","        return pth_files\n","\n","    print(f\"pth 파일 검색 시작: {base_path}\")\n","\n","    # 하위 폴더들을 순회하며 pth 파일 검색\n","    for root, dirs, files in os.walk(base_path):\n","        for file in files:\n","            if file.endswith('.pth'):\n","                pth_path = os.path.join(root, file)\n","                pth_files.append(pth_path)\n","\n","    # 결과 정리 및 출력\n","    if pth_files:\n","        print(f\"\\n발견된 pth 파일들 ({len(pth_files)}개):\")\n","        for i, pth_file in enumerate(pth_files, 1):\n","            # 상대 경로로 표시 (base_path 기준)\n","            rel_path = os.path.relpath(pth_file, base_path)\n","            print(f\" {i:2d}. {rel_path}\")\n","    else:\n","        print(\"pth 파일을 찾을 수 없습니다.\")\n","\n","    return pth_files\n","\n","def print_json_tree(data, indent=\"\", max_depth=4, _depth=0, list_count=2, print_value=True):\n","    \"\"\"\n","    JSON 객체를 지정한 단계(max_depth)까지 트리 형태로 출력\n","    - list 타입은 3개 이상일 때 개수만 출력\n","    - 하위 노드가 값일 경우 key(type) 형태로 출력\n","    - print_value=True일 때 key(type): 값 형태로 출력\n","    \"\"\"\n","    if _depth > max_depth:\n","        return\n","    if isinstance(data, dict):\n","        for key, value in data.items():\n","            if isinstance(value, (dict, list)):\n","                print(f\"{indent}|-- {key}\")\n","                print_json_tree(value, indent + \"    \", max_depth, _depth + 1, list_count, print_value)\n","            else:\n","                if print_value:\n","                    print(f\"{indent}|-- {key}({type(value).__name__}): {value if len(str(value)) < 100 else f'{str(value)[:30]}...'}\")\n","                else:\n","                    print(f\"{indent}|-- {key}({type(value).__name__})\")\n","    elif isinstance(data, list):\n","        if len(data) > list_count:\n","            print(f\"{indent}|-- [list] ({len(data)} items)\")\n","        else:\n","            for i, item in enumerate(data):\n","                if isinstance(item, (dict, list)):\n","                    print(f\"{indent}|-- [{i}]\")\n","                    print_json_tree(item, indent + \"    \", max_depth, _depth + 1, list_count, print_value)\n","                else:\n","                    if print_value:\n","                        print(f\"{indent}|-- [{i}]({type(item).__name__}): {item if len(str(item)) < 100 else f'{str(item)[:30]}...'}\")\n","                    else:\n","                        print(f\"{indent}|-- [{i}]({type(item).__name__})\")\n","    else:\n","        if print_value:\n","            print(f\"{indent}{type(data).__name__}: {data if len(str(data)) < 100 else f'{str(data)[:30]}...'}\")\n","        else:\n","            print(f\"{indent}{type(data).__name__}\")\n","\n","def print_git_tree(data, indent=\"\", max_depth=3, _depth=0):\n","    \"\"\"\n","    PyTorch tensor/딕셔너리/리스트를 git tree 스타일로 출력\n","    \"\"\"\n","    import torch\n","    import numpy as np\n","\n","    if _depth > max_depth:\n","        return\n","    if isinstance(data, dict):\n","        for key, value in data.items():\n","            print(f\"{indent}├─ {key} [{type(value).__name__}]\")\n","            print_git_tree(value, indent + \"│  \", max_depth, _depth + 1)\n","    elif isinstance(data, (list, tuple)):\n","        for i, item in enumerate(data):\n","            print(f\"{indent}├─ [{i}] [{type(item).__name__}]\")\n","            print_git_tree(item, indent + \"│  \", max_depth, _depth + 1)\n","    elif torch.is_tensor(data):\n","        shape = tuple(data.shape)\n","        dtype = str(data.dtype)\n","        preview = str(data)\n","        preview_str = preview[:80] + (\"...\" if len(preview) > 80 else \"\")\n","        print(f\"{indent}└─ Tensor shape={shape} dtype={dtype} preview={preview_str}\")\n","    elif isinstance(data, np.ndarray):\n","        shape = data.shape\n","        dtype = data.dtype\n","        preview = str(data)\n","        preview_str = preview[:80] + (\"...\" if len(preview) > 80 else \"\")\n","        print(f\"{indent}└─ ndarray shape={shape} dtype={dtype} preview={preview_str}\")\n","    else:\n","        val_str = str(data)\n","        print(f\"{indent}└─ {type(data).__name__}: {val_str[:80]}{'...' if len(val_str)>80 else ''}\")\n","\n","\n","print(\"유틸리티 함수 로드 완료\")"]},{"cell_type":"markdown","metadata":{"id":"1B4qELHp5E9D"},"source":["# 데이타 다운로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0o9rB50tzK-A"},"outputs":[],"source":["# download_files={\n","#     'yolo_label_one_class' : r'https://drive.google.com/file/d/177_86k4BuT6JnFnq7ZHJtEjp7jaRbCl2/view?usp=sharing',\n","#     'yolo_label' : r'https://drive.google.com/file/d/1nc-WFcw7lCS7s7VGzN9Kxh80PiBBggez/view?usp=sharing',\n","#     'yolo_resize_one_class' : r'https://drive.google.com/file/d/1Ak0EvkMnuwvcAFvTO-zovIgVcNlROjsS/view?usp=sharing',\n","#     'yolo_resize' : r'https://drive.google.com/file/d/1kpo57qOJhEhrkuzUCEh57ILB5xSPVoFv/view?usp=sharing',\n","# }\n","\n","# download_files={\n","#     'yolo_label' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODU5OTkzNTMyOHxGfDA&svcType=MYBOX-WEB&time=1757776010785',\n","#     'yolo_label_one_class' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODYzOTg5NDExMnxGfDA&svcType=MYBOX-WEB&time=1757776673721',\n","#     'yolo_resize_one_class' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODgwNjk2NDMyMHxGfDA&svcType=MYBOX-WEB&time=1757780142635',\n","#     'yolo_resize' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODY4MDc2MjQ2NHxGfDA&svcType=MYBOX-WEB&time=1757780177672',\n","# }\n","\n","download_files={\n","    # 'yolo_label' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODU5OTkzNTMyOHxGfDA&svcType=MYBOX-WEB&time=1757776010785',\n","    # 'yolo_label_one_class' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODYzOTg5NDExMnxGfDA&svcType=MYBOX-WEB&time=1757776673721',\n","    # 'yolo_resize_one_class' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODgwNjk2NDMyMHxGfDA&svcType=MYBOX-WEB&time=1757780142635',\n","    # 'yolo_resize' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc1ODY4MDc2MjQ2NHxGfDA&svcType=MYBOX-WEB&time=1757780177672',\n","    'yolo_noresize' : r'https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc2NDA0ODY2ODI1NnxGfDA&svcType=MYBOX-WEB&time=1757851996107',\n","}\n","# yolo_noresize = https://fs.mybox.naver.com/file/download.api?resourceKey=YzB6MGN8MzQ3MjU5Nzc2NDA0ODY2ODI1NnxGfDA&svcType=MYBOX-WEB&time=1757851996107\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdLVsfpwzK-A"},"outputs":[],"source":["import gdown\n","def download_gdrive_file(url, output_path, ignore=True):\n","    # 공유 링크에서 파일 ID 추출\n","    if os.path.exists(output_path):\n","        if ignore:\n","            os.remove(output_path)\n","        else:\n","            return\n","\n","    file_id_match = re.search(r'/d/([a-zA-Z0-9_-]+)', url)\n","    if not file_id_match:\n","        raise ValueError(\"Google Drive 파일 ID를 찾을 수 없습니다.\")\n","    file_id = file_id_match.group(1)\n","    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n","\n","def download_http(url, target, ignore=True):\n","    \"\"\"\n","    HTTP 파일 다운로드 함수 (진행률 표시)\n","    url: 다운로드할 파일 URL\n","    target: 저장할 파일 경로\n","    ignore: True면 기존 파일 삭제 후 다운로드, False면 파일 있으면 건너뜀\n","    \"\"\"\n","    if os.path.exists(target):\n","        if ignore:\n","            os.remove(target)\n","        else:\n","            print(f\"이미 파일이 존재합니다: {target}\")\n","            return target\n","\n","    response = requests.get(url, stream=True)\n","    total = int(response.headers.get('content-length', 0))\n","    with open(target, 'wb') as file, tqdm(\n","        desc=f\"Downloading {os.path.basename(target)}\",\n","        total=total,\n","        unit='B',\n","        unit_scale=True,\n","        unit_divisor=1024,\n","        ascii=True\n","    ) as bar:\n","        for data in response.iter_content(chunk_size=1024):\n","            size = file.write(data)\n","            bar.update(size)\n","    print(f\"다운로드 완료: {target}\")\n","    return target\n","\n","# local_code_it_ai04 = os.path.join( '~/.cache/' if helper.is_colab else Path.cwd().drive + '\\\\'\n","#                                   ,'temp'\n","#                                   , 'code_it_ai04')\n","\n","if helper.is_colab:\n","    local_code_it_ai04 = os.path.join( '/content/', 'code_it_ai04')\n","else:\n","    local_code_it_ai04 = os.path.join( Path.cwd().drive + '\\\\', 'temp', 'code_it_ai04')\n","\n","print(\"local_code_it_ai04:\", local_code_it_ai04)\n","\n","os.makedirs(local_code_it_ai04, exist_ok=True)  # 폴더 생성 코드 추가\n","unzip_paths = []\n","for key, url in download_files.items():\n","    print(f\"{key}: {url}\")\n","    zipfile = os.path.join(local_code_it_ai04, f'{key}.zip')\n","    unzip_path = os.path.join(local_code_it_ai04, f'{key}.zip.unzip')\n","    if os.path.exists(unzip_path):\n","        print(f\"이미 압축해제된 폴더가 존재합니다: {unzip_path}\")\n","        print('unzipfile:', unzip_path)\n","        unzip_paths.append(unzip_path)\n","        continue\n","    #download_gdrive_file(url, os.path.join(local_code_it_ai04, f'{key}.zip'), ignore=False)\n","    download_http(url, zipfile, ignore=False)\n","    unzip_path_list = heu.unzip([os.path.join(local_code_it_ai04, f'{key}.zip')])\n","    # for p in unzip_path_list:\n","    #     unzip_paths.append(p)\n","    print('unzip_path_list:', unzip_path_list)\n","    unzip_paths.extend(unzip_path_list)\n"]},{"cell_type":"markdown","metadata":{"id":"JfapF4EsMlGp"},"source":["### > 설정 < 플레그"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjxsUVjUMlGq"},"outputs":[],"source":["# google drive root에 keggle.json 파일 필요합니다.\n","for path in unzip_paths:\n","    print(\"압축해제된 폴더:\", path)\n","\n","#yolo_dataset_path = os.path.join(local_code_it_ai04, f'yolo_label_one_class.zip.unzip')\n","yolo_dataset_path =unzip_paths[0]\n","yaml_path = os.path.join(yolo_dataset_path, \"dataset.yaml\")\n","\n","def get_path_data():\n","    path = yolo_dataset_path\n","    return path\n","\n","print(\"yaml_path:\", yaml_path)\n","print(\"get_path_data:\", get_path_data())"]},{"cell_type":"markdown","metadata":{"id":"ZRk0o5CUMlGq"},"source":["## EfficientNet 모델링"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jv-fWI-wzK-B"},"outputs":[],"source":["for path in unzip_paths:\n","    print(f\"압축해제된 폴더: {path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Z6vnx9ANq4u"},"outputs":[],"source":["class YOLOToClassificationDataset(Dataset):\n","    \"\"\"YOLO 형식 데이터를 분류용으로 변환하는 데이터셋\"\"\"\n","\n","    def __init__(self, yaml_path, split='train', transform=None, crop_objects=True):\n","        # yaml 파일 읽기\n","        with open(yaml_path, 'r') as f:\n","            yaml_data = yaml.safe_load(f)\n","\n","        self.yaml_data_path = yaml_data['path']\n","        self.nc = yaml_data['nc']\n","        self.names = yaml_data['names']\n","        self.yaml_data_train = os.path.join(yaml_data['path'], yaml_data['train'])\n","        self.yaml_data_val = os.path.join(yaml_data['path'], yaml_data['val'])\n","        self.yaml_data_test = os.path.join(yaml_data['path'], yaml_data['test'])\n","\n","        #print(\"nc:\", self.nc)\n","        #print(\"names:\", self.names)\n","        print(\"yaml_data_path:\", self.yaml_data_path)\n","        #print(\"yaml_data_train:\", self.yaml_data_train)\n","        #print(\"yaml_data_val:\", self.yaml_data_val)\n","        #print(\"yaml_data_test:\", self.yaml_data_test)\n","\n","        # split에 따라 경로 선택\n","        if split == 'train':\n","            image_dir = self.yaml_data_train\n","        elif split == 'val':\n","            image_dir = self.yaml_data_val\n","        elif split == 'test':\n","            image_dir = self.yaml_data_test\n","        else:\n","            raise ValueError(f\"split 값이 잘못되었습니다: {split}\")\n","\n","        label_dir = image_dir.replace('images', 'labels')\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.class_names = self.names\n","        self.transform = transform\n","        self.crop_objects = crop_objects\n","        self.data = []\n","\n","        self._load_data()\n","\n","    def _load_data(self):\n","        if not os.path.exists(self.label_dir):\n","            #print(f\"라벨 폴더가 존재하지 않습니다: {self.label_dir}\")\n","            return  # 라벨 폴더 없으면 빈 데이터셋\n","        for label_file in os.listdir(self.label_dir):\n","            if not label_file.endswith('.txt'):\n","                continue\n","            image_file = label_file.replace('.txt', '.jpg')\n","            image_path = os.path.join(self.image_dir, image_file)\n","            label_path = os.path.join(self.label_dir, label_file)\n","            if not os.path.exists(image_path):\n","                continue\n","            with open(label_path, 'r') as f:\n","                lines = f.readlines()\n","            for line in lines:\n","                parts = line.strip().split()\n","                if len(parts) >= 5:\n","                    class_id = int(parts[0])\n","                    x_center, y_center, width, height = map(float, parts[1:5])\n","                    self.data.append({\n","                        'image_path': image_path,\n","                        'class_id': class_id,\n","                        'bbox': (x_center, y_center, width, height)\n","                    })\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        image = Image.open(item['image_path']).convert('RGB')\n","        if self.crop_objects and 'bbox' in item:\n","            img_w, img_h = image.size\n","            x_center, y_center, width, height = item['bbox']\n","            x1 = int((x_center - width/2) * img_w)\n","            y1 = int((y_center - height/2) * img_h)\n","            x2 = int((x_center + width/2) * img_w)\n","            y2 = int((y_center + height/2) * img_h)\n","            # 좌표가 올바른지 체크\n","            if x2 > x1 and y2 > y1 and x1 >= 0 and y1 >= 0 and x2 <= img_w and y2 <= img_h:\n","                image = image.crop((x1, y1, x2, y2))\n","            # else: 잘못된 bbox는 crop하지 않음\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, item['class_id']\n","    def get_item(self, idx):\n","        item = self.data[idx]\n","        image = Image.open(item['image_path']).convert('RGB')\n","        if self.crop_objects and 'bbox' in item:\n","            img_w, img_h = image.size\n","            x_center, y_center, width, height = item['bbox']\n","            x1 = int((x_center - width/2) * img_w)\n","            y1 = int((y_center - height/2) * img_h)\n","            x2 = int((x_center + width/2) * img_w)\n","            y2 = int((y_center + height/2) * img_h)\n","            # 좌표가 올바른지 체크\n","            if x2 > x1 and y2 > y1 and x1 >= 0 and y1 >= 0 and x2 <= img_w and y2 <= img_h:\n","                image = image.crop((x1, y1, x2, y2))\n","            # else: 잘못된 bbox는 crop하지 않음\n","        return image, item['class_id'], item['image_path'], [x1, y1, x2, y2]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lj7vfQNHNq4v"},"outputs":[],"source":["\n","# 데이터 전처리 파이프라인\n","def get_transforms():\n","    train_transform = transforms.Compose([\n","        transforms.Resize((300, 300)),  # EfficientNet-B3 적절한 크기\n","        transforms.RandomHorizontalFlip(0.5),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                           std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    val_transform = transforms.Compose([\n","        transforms.Resize((300, 300)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                           std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    return train_transform, val_transform\n","\n","# EfficientNet-B3 모델 생성\n","def create_efficientnet_model(num_classes, pretrained=True):\n","    \"\"\"사전 학습된 EfficientNet-B3 모델 로드\"\"\"\n","    model = timm.create_model('efficientnet_b3',\n","                             pretrained=pretrained,\n","                             num_classes=num_classes)\n","    return model\n","\n","# 학습 함수\n","def train_epoch(model, dataloader, criterion, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    pbar = tqdm(dataloader, desc='Training')\n","    for images, labels in pbar:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        pbar.set_postfix({\n","            'Loss': f'{running_loss/len(dataloader):.4f}',\n","            'Acc': f'{100*correct/total:.2f}%'\n","        })\n","\n","    return running_loss/len(dataloader), 100*correct/total\n","\n","# 검증 함수\n","def validate(model, dataloader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(dataloader, desc='Validating'):\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    return running_loss/len(dataloader), 100*correct/total\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0UX5QuCUPiM"},"outputs":[],"source":["def update_yaml_paths_to_absolute(yaml_path):\n","    with open(yaml_path, 'r') as f:\n","        data = yaml.safe_load(f)\n","\n","    yaml_dir = os.path.dirname(yaml_path)\n","    data['path'] = os.path.normpath(os.path.join(yaml_dir, data['path']))\n","    # for key in ['train', 'val', 'test']:\n","    #     if key in data and not os.path.isabs(data[key]):\n","    #         data[key] = os.path.normpath(os.path.join(yaml_dir, data[key]))\n","\n","    with open(yaml_path, 'w') as f:\n","        yaml.dump(data, f, allow_unicode=True)\n","\n","    return data\n","\n","yaml_data = update_yaml_paths_to_absolute(yaml_path)\n","print(yaml_data.keys())\n","yaml_data_path = yaml_data['path']\n","nc = yaml_data['nc']\n","names = yaml_data['names']\n","yaml_data_train = os.path.join(yaml_data['path'], yaml_data['train'])\n","yaml_data_val = os.path.join(yaml_data['path'], yaml_data['val'])\n","yaml_data_test = os.path.join(yaml_data['path'], yaml_data['test'])\n","\n","print(\"nc:\", nc)\n","print(\"names:\", names)\n","print(\"yaml_data_path:\", yaml_data_path)\n","print(\"yaml_data_train:\", yaml_data_train)\n","print(\"yaml_data_val:\", yaml_data_val)\n","print(\"yaml_data_test:\", yaml_data_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gth-UMvZUPiN"},"outputs":[],"source":["# raise ValueError(\"중단\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDJCLAC2z4q9"},"outputs":[],"source":["# sample image\n","def save_sample(dataset, save_dir, type, num_samples=5):\n","    os.makedirs(save_dir, exist_ok=True)\n","    pbar = tqdm(range(min(num_samples, len(dataset))), desc=\"샘플 이미지 저장 중\")\n","    for i in pbar:\n","        image, label, class_image_path, bbox = dataset.get_item(i)\n","        base_file_name = os.path.basename(class_image_path).split('.')[0]\n","        class_name = dataset.class_names[label]\n","        image_path = os.path.join(save_dir, 'images', type, f'{base_file_name}_{class_name}.jpg')\n","        os.makedirs(os.path.dirname(image_path), exist_ok=True)  # 폴더 생성 추가\n","        # 텐서를 PIL 이미지로 변환\n","        if isinstance(image, torch.Tensor):\n","            image = TF.to_pil_image(image)\n","        image.save(image_path)\n","        label_path = os.path.join(save_dir, 'labels', type, f'{base_file_name}_{class_name}.txt')\n","        os.makedirs(os.path.dirname(label_path), exist_ok=True)  # 폴더 생성 추가\n","        with open(label_path, 'w') as f:\n","            f.write(f\"{label}\\n\")\n","        pbar.set_postfix_str(f'{base_file_name}_{class_name}')\n","\n","def save_classification_yaml(save_dir, class_names):\n","    yaml_dict = {\n","        'path': '.',\n","        'train': 'images/train',\n","        'val': 'images/val',\n","        'test': 'images/test',\n","        'nc': len(class_names),\n","        'names': class_names\n","    }\n","    yaml_path = os.path.join(save_dir, 'dataset.yaml')\n","    with open(yaml_path, 'w', encoding='utf-8') as f:\n","        yaml.dump(yaml_dict, f, allow_unicode=True)\n","    print(f'dataset.yaml 저장: {yaml_path}')\n","\n","def save_yaml_sample_images():\n","    yaml_data = update_yaml_paths_to_absolute(yaml_path)\n","    print(yaml_data.keys())\n","    yaml_data_path = yaml_data['path']\n","    nc = yaml_data['nc']\n","    names = yaml_data['names']\n","    yaml_data_train = os.path.join(yaml_data['path'], yaml_data['train'])\n","    yaml_data_val = os.path.join(yaml_data['path'], yaml_data['val'])\n","    yaml_data_test = os.path.join(yaml_data['path'], yaml_data['test'])\n","\n","\n","    CLASS_NAMES = names\n","    NUM_CLASSES = len(CLASS_NAMES)\n","    BATCH_SIZE = 16\n","    LEARNING_RATE = 0.001\n","    NUM_EPOCHS = 10\n","    # ========================================\n","\n","    # 데이터셋 및 데이터로더 생성\n","    train_transform, val_transform = get_transforms()\n","\n","    # 전체 데이터셋 (실제로는 train/val 분할 필요)\n","    train_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='train',\n","        transform=train_transform\n","    )\n","    val_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='val',\n","        transform=val_transform\n","    )\n","    test_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='test',\n","        transform=val_transform\n","    )\n","    yaml_data_sample_images_train_path = os.path.join(f\"{yaml_data_path}_one\")\n","    yaml_data_sample_images_val_path = os.path.join(f\"{yaml_data_path}_one\")\n","    save_sample(train_dataset, yaml_data_sample_images_train_path, 'train', num_samples=len(train_dataset))\n","    save_sample(val_dataset, yaml_data_sample_images_val_path, 'val', num_samples=len(val_dataset))\n","    save_classification_yaml(yaml_data_sample_images_train_path, CLASS_NAMES)\n","    print(\"샘플 이미지 저장 완료\")\n","\n","# 알약 별로 이미지를 분리한 후 저장\n","# save_yaml_sample_images()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6j6R7sLz4q9"},"outputs":[],"source":["# raise ValueError(\"중단\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC2UYJQvNq4v"},"outputs":[],"source":["\n","# 메인 실행 함수\n","def main_efficientnet():\n","    # ========== 설정 부분 (수정 필요) ==========\n","    yaml_data = update_yaml_paths_to_absolute(yaml_path)\n","    print(yaml_data.keys())\n","    yaml_data_path = yaml_data['path']\n","    nc = yaml_data['nc']\n","    names = yaml_data['names']\n","    yaml_data_train = os.path.join(yaml_data['path'], yaml_data['train'])\n","    yaml_data_val = os.path.join(yaml_data['path'], yaml_data['val'])\n","    yaml_data_test = os.path.join(yaml_data['path'], yaml_data['test'])\n","\n","    CLASS_NAMES = names\n","    NUM_CLASSES = len(CLASS_NAMES)\n","    BATCH_SIZE = 16\n","    LEARNING_RATE = 0.001\n","    NUM_EPOCHS = 10\n","\n","    # 모델 저장 경로 설정 (YOLO와 동일한 구조)\n","    model_save_name = f\"efficientnet_b3_cls_{NUM_CLASSES}classes\"\n","    model_save_dir = get_path_modeling()\n","    model_project_dir = os.path.join(model_save_dir, model_save_name)\n","    os.makedirs(model_project_dir, exist_ok=True)\n","\n","    print(f\"모델 저장 경로: {model_project_dir}\")\n","    # ========================================\n","\n","    # 데이터셋 및 데이터로더 생성\n","    train_transform, val_transform = get_transforms()\n","\n","    train_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='train',\n","        transform=train_transform\n","    )\n","    val_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='val',\n","        transform=val_transform\n","    )\n","    test_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='test',\n","        transform=val_transform\n","    )\n","\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    print(f\"Training samples: {len(train_dataset)}\")\n","    print(f\"Validation samples: {len(val_dataset)}\")\n","    print(f\"Test samples: {len(test_dataset)}\")\n","\n","    # 모델 생성\n","    model = create_efficientnet_model(NUM_CLASSES, pretrained=True)\n","    model = model.to(__device)\n","\n","    # 손실 함수와 옵티마이저\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    # 학습 루프\n","    best_val_acc = 0.0\n","    train_losses, train_accs = [], []\n","    val_losses, val_accs = [], []\n","\n","    for epoch in range(NUM_EPOCHS):\n","        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n","        print(\"-\" * 50)\n","\n","        # 학습\n","        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, __device)\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","\n","        # 검증\n","        val_loss, val_acc = validate(model, val_loader, criterion, __device)\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","\n","        scheduler.step()\n","\n","        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n","        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n","\n","        # 최고 성능 모델 저장 (YOLO 스타일로 수정)\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","\n","            # 모델 정보 구성\n","            model_info = {\n","                'model_name': 'efficientnet_b3',\n","                'num_classes': NUM_CLASSES,\n","                'class_names': CLASS_NAMES,\n","                'best_val_acc': val_acc,\n","                'epoch': epoch + 1,\n","                'learning_rate': LEARNING_RATE,\n","                'batch_size': BATCH_SIZE,\n","                'train_samples': len(train_dataset),\n","                'val_samples': len(val_dataset),\n","                'test_samples': len(test_dataset),\n","            }\n","\n","            # 베스트 모델 저장\n","            best_model_path = save_model_dict(\n","                model=model,\n","                path=model_project_dir,\n","                pth_name='best',\n","                kwargs=model_info\n","            )\n","\n","            print(f\"Best model saved! Val Acc: {val_acc:.2f}%\")\n","            print(f\"저장 경로: {best_model_path}\")\n","\n","    # 최종 모델도 저장\n","    final_model_info = model_info.copy()\n","    final_model_info.update({\n","        'final_val_acc': val_acc,\n","        'final_epoch': NUM_EPOCHS,\n","        'training_completed': True\n","    })\n","\n","    final_model_path = save_model_dict(\n","        model=model,\n","        path=model_project_dir,\n","        pth_name='final',\n","        kwargs=final_model_info\n","    )\n","\n","    print(f\"Final model saved: {final_model_path}\")\n","\n","    # 결과 시각화 및 저장\n","    plt.figure(figsize=(12, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Val Loss')\n","    plt.title('Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accs, label='Train Acc')\n","    plt.plot(val_accs, label='Val Acc')\n","    plt.title('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","\n","    # 차트도 같은 폴더에 저장\n","    chart_path = os.path.join(model_project_dir, 'training_results.png')\n","    plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n","    print(f\"Training chart saved: {chart_path}\")\n","\n","    plt.show()\n","\n","    return model_project_dir, best_val_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LmtT_cp14nQ"},"outputs":[],"source":["import wandb\n","\n","def main_efficientnet_with_wandb(project_name=\"codeit-ai-04-01\", run_name=None):\n","    # ========== 설정 부분 (수정 필요) ==========\n","    yaml_data = update_yaml_paths_to_absolute(yaml_path)\n","    print(yaml_data.keys())\n","    yaml_data_path = yaml_data['path']\n","    nc = yaml_data['nc']\n","    names = yaml_data['names']\n","    yaml_data_train = os.path.join(yaml_data['path'], yaml_data['train'])\n","    yaml_data_val = os.path.join(yaml_data['path'], yaml_data['val'])\n","    yaml_data_test = os.path.join(yaml_data['path'], yaml_data['test'])\n","\n","    CLASS_NAMES = names\n","    NUM_CLASSES = len(CLASS_NAMES)\n","    BATCH_SIZE = 16\n","    LEARNING_RATE = 0.001\n","    NUM_EPOCHS = 10\n","\n","    # 모델 저장 경로 설정 (YOLO와 동일한 구조)\n","    model_save_name = f\"efficientnet_b3_cls_{NUM_CLASSES}classes\"\n","    model_save_dir = get_path_modeling()\n","    model_project_dir = os.path.join(model_save_dir, model_save_name)\n","    os.makedirs(model_project_dir, exist_ok=True)\n","\n","    print(f\"모델 저장 경로: {model_project_dir}\")\n","\n","    # wandb 초기화\n","    wandb_config = {\n","        'model_name': 'efficientnet_b3',\n","        'num_classes': NUM_CLASSES,\n","        'class_names': CLASS_NAMES,\n","        'batch_size': BATCH_SIZE,\n","        'learning_rate': LEARNING_RATE,\n","        'num_epochs': NUM_EPOCHS,\n","        'image_size': 300,\n","        'optimizer': 'Adam',\n","        'scheduler': 'StepLR',\n","        'pretrained': True\n","    }\n","\n","    run = wandb.init(\n","        project=project_name,\n","        name=run_name or f\"efficientnet_b3_{NUM_CLASSES}classes_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n","        config=wandb_config,\n","        tags=['efficientnet', 'classification', 'pytorch']\n","    )\n","    # ========================================\n","\n","    # 데이터셋 및 데이터로더 생성\n","    train_transform, val_transform = get_transforms()\n","\n","    train_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='train',\n","        transform=train_transform\n","    )\n","    val_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='val',\n","        transform=val_transform\n","    )\n","    test_dataset = YOLOToClassificationDataset(\n","        yaml_path = yaml_path,\n","        split='test',\n","        transform=val_transform\n","    )\n","\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    print(f\"Training samples: {len(train_dataset)}\")\n","    print(f\"Validation samples: {len(val_dataset)}\")\n","    print(f\"Test samples: {len(test_dataset)}\")\n","\n","    # wandb에 데이터셋 정보 로깅\n","    wandb.log({\n","        'dataset/train_samples': len(train_dataset),\n","        'dataset/val_samples': len(val_dataset),\n","        'dataset/test_samples': len(test_dataset),\n","        'dataset/total_samples': len(train_dataset) + len(val_dataset) + len(test_dataset)\n","    })\n","\n","    # 모델 생성\n","    model = create_efficientnet_model(NUM_CLASSES, pretrained=True)\n","    model = model.to(__device)\n","\n","    # wandb에 모델 구조 로깅\n","    wandb.watch(model, log='all', log_freq=100)\n","\n","    # 손실 함수와 옵티마이저\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","    # 학습 루프\n","    best_val_acc = 0.0\n","    train_losses, train_accs = [], []\n","    val_losses, val_accs = [], []\n","\n","    for epoch in range(NUM_EPOCHS):\n","        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n","        print(\"-\" * 50)\n","\n","        # 학습\n","        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, __device)\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","\n","        # 검증\n","        val_loss, val_acc = validate(model, val_loader, criterion, __device)\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","\n","        scheduler.step()\n","\n","        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n","        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n","\n","        # wandb에 메트릭 로깅\n","        wandb.log({\n","            'epoch': epoch + 1,\n","            'train/loss': train_loss,\n","            'train/accuracy': train_acc,\n","            'val/loss': val_loss,\n","            'val/accuracy': val_acc,\n","            'learning_rate': optimizer.param_groups[0]['lr']\n","        }, step=epoch)\n","\n","        # 최고 성능 모델 저장\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","\n","            # 모델 정보 구성\n","            model_info = {\n","                'model_name': 'efficientnet_b3',\n","                'num_classes': NUM_CLASSES,\n","                'class_names': CLASS_NAMES,\n","                'best_val_acc': val_acc,\n","                'epoch': epoch + 1,\n","                'learning_rate': LEARNING_RATE,\n","                'batch_size': BATCH_SIZE,\n","                'train_samples': len(train_dataset),\n","                'val_samples': len(val_dataset),\n","                'test_samples': len(test_dataset),\n","            }\n","\n","            # 베스트 모델 저장\n","            best_model_path = save_model_dict(\n","                model=model,\n","                path=model_project_dir,\n","                pth_name='best',\n","                kwargs=model_info\n","            )\n","\n","            print(f\"Best model saved! Val Acc: {val_acc:.2f}%\")\n","            print(f\"저장 경로: {best_model_path}\")\n","\n","            # wandb에 베스트 모델 저장\n","            wandb.log({'best_val_accuracy': val_acc, 'best_epoch': epoch + 1})\n","\n","    # 최종 모델도 저장\n","    final_model_info = model_info.copy()\n","    final_model_info.update({\n","        'final_val_acc': val_acc,\n","        'final_epoch': NUM_EPOCHS,\n","        'training_completed': True\n","    })\n","\n","    final_model_path = save_model_dict(\n","        model=model,\n","        path=model_project_dir,\n","        pth_name='final',\n","        kwargs=final_model_info\n","    )\n","\n","    print(f\"Final model saved: {final_model_path}\")\n","\n","    # 결과 시각화 및 저장\n","    plt.figure(figsize=(12, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Val Loss')\n","    plt.title('Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accs, label='Train Acc')\n","    plt.plot(val_accs, label='Val Acc')\n","    plt.title('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","\n","    # 차트도 같은 폴더에 저장\n","    chart_path = os.path.join(model_project_dir, 'training_results.png')\n","    plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n","    print(f\"Training chart saved: {chart_path}\")\n","\n","    # wandb에 차트 업로드\n","    wandb.log({\"training_results\": wandb.Image(chart_path)})\n","\n","    plt.show()\n","\n","    # 최종 요약 메트릭\n","    wandb.log({\n","        'final/best_val_accuracy': best_val_acc,\n","        'final/final_val_accuracy': val_acc,\n","        'final/total_epochs': NUM_EPOCHS\n","    })\n","\n","    # 모델 아티팩트 저장\n","    artifact = wandb.Artifact(\n","        name=f\"efficientnet_b3_model_{run.id}\",\n","        type=\"model\",\n","        description=f\"EfficientNet-B3 model trained on {NUM_CLASSES} classes\"\n","    )\n","    artifact.add_file(best_model_path)\n","    artifact.add_file(final_model_path)\n","    artifact.add_file(chart_path)\n","    wandb.log_artifact(artifact)\n","\n","    # wandb 실행 종료\n","    wandb.finish()\n","\n","    return model_project_dir, best_val_acc\n","\n","# 실행 함수\n","def run_efficientnet_with_wandb():\n","    try:\n","        project_dir, best_acc = main_efficientnet_with_wandb(\n","            project_name=\"codeit-ai-04-01\",\n","            run_name=f\"efficientnet_b3_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","        )\n","        print(f\"학습 완료! 최고 검증 정확도: {best_acc:.2f}%\")\n","        print(f\"모델 저장 경로: {project_dir}\")\n","        return project_dir, best_acc\n","    except Exception as e:\n","        print(f\"학습 중 오류 발생: {e}\")\n","        wandb.finish()\n","        raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFUs6ByDN0ll"},"outputs":[],"source":["# raise ValueError('stop end')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUHwajzLNq4v"},"outputs":[],"source":["#main_efficientnet()\n","run_efficientnet_with_wandb()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcz6L6ciz4q-"},"outputs":[],"source":["def test_model(model_path, image_path:list):\n","    \"\"\"저장된 모델을 불러와서 테스트 이미지에 대해 예측 수행\"\"\"\n","    model_state, model_info = load_model_dict(model_path, pth_name='best')\n","    print(\"모델 정보:\", model_info)\n","\n","    num_classes = model_info['num_classes']\n","    class_names = model_info['class_names']\n","\n","    # 모델 생성 및 state_dict 로드\n","    model = create_efficientnet_model(num_classes, pretrained=False)\n","    model.load_state_dict(model_state)\n","    model = model.to(__device)\n","    model.eval()\n","\n","    # 이미지 전처리\n","    transform = transforms.Compose([\n","        transforms.Resize((300, 300)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                           std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    for img_path in image_path:\n","        image = Image.open(img_path).convert('RGB')\n","        input_tensor = transform(image).unsqueeze(0).to(__device)\n","\n","        with torch.no_grad():\n","            output = model(input_tensor)\n","            _, predicted = torch.max(output, 1)\n","            predicted_class = class_names[predicted.item()]\n","\n","        plt.imshow(image)\n","        plt.title(f'Predicted: {predicted_class}')\n","        plt.axis('off')\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04KFMQG7z4q-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJLu81VDz4q-"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"env_colab_250827","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}